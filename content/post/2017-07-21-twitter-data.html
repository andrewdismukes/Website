---
title: Twitter Data
author: Andrew Dismukes
date: '2017-07-21'
slug: twitter-data
categories:
  - Methods
tags:
  - Twitter
type: ''
subtitle: ''
image: ''
editor_options: 
  chunk_output_type: inline
---



<p><em>Update November 2018: You now need to apply for a developer account to get a twitter API token.</em></p>
<p>I’m not very good with words.</p>
<p>At least from an analytical sense - I’m more comfortable with numbers. I say this not to criticize qualitative work, but to be up front with my limitations. Lately, however, I’ve discovered the twitter API, and it’s fun to play with. This is a lark, but it’s pushed me to learn a few new tricks in R, and to work with packages I haven’t used before, so a learning-lark.</p>
<p>It’s actually not hard to get twitter data, which surprised me (although given the number of data sources that are just sitting around, from the 538 github account to the US census, it probably shouldn’t have).</p>
<p>You need to be on twitter to get the data, so sign up if you haven’t done so. You don’t have to use it. The next step is to register a new twitter app here and log in with your new (or old) twitter user name and password. This ‘app’ is just to get an api key. Click on the create a new app button and fill in the required info - give the app a name and description, and provide a website (you do need to add a website; there are lots of ways to get a domain or you can just make one up. Or register with netlify; I’ll have a post about their service and website creation using R markdown coming up soon.) Use <a href="http://127.0.0.1:1410" class="uri">http://127.0.0.1:1410</a> for the callback url.</p>
<p>This will provide you with an api key and api secret from twitter… and you are ready to go to get data.</p>
<p>From within R, you’ll need a few packages installed and loaded:</p>
<p>And then you will need to log in to twitter using those new credentials you just got, by executing this command in R but replacing the quotes with your twitter-provided api credentials:</p>
<p>Now you’re ready to go- it really is surprisingly easy. So…I wanted to take a look at word clouds, frequencies, etc., as a learning exercise. The first thing to do was to get some data:</p>
<p>What I’ve done is create an object by searching twitter for the phrase ‘#maga’, returning 10000 tweets, in the last 24 hours. I’ve turned that into a data frame and set up time to match local.</p>
<p>Now:</p>
<p>This returns a tokenized tibble. A token, in this case, is a word. Each word is now a single unit of analysis nested within twitter user - each tweet will now be many rows long. Our data set has gotten much, much bigger (mine was 125K rows long). But, this ‘tidy text’, stripped of punctuation and with some other useful manipulations (see here for documentation on this excellent package and function unnest_tokens) is much easier to work with to extract signal from the noise of twitter. I’m assuming a familiarity with both tidyverse and base R.</p>
<p>Another trick of tidytext is to strip filler words, so we’ll take care of that too:</p>
<p>Now we have a data frame of individual words nested within user stripped of punctuation and filler, ready to graph:</p>
<p>To follow the pipe, we’ve taken the tibble ‘long’ of tweets containing ‘#maga’, counted up the number of times a word appeared, removed just numbers, filtered out some twitter and hashtag specific terms with grepl, and piped that into a wordcloud of the top 100 words contained in tweets that use the hashtag maga, which looks like (as of 9 pm Wednesday November 21 2018):</p>
