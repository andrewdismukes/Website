---
title: Git for RR workflow, part two
author: Andrew Dismukes
date: '2017-08-18'
slug: git-for-rr-workflow-part-two
categories:
  - Methods
tags:
  - Reproducible Research
type: ''
subtitle: ''
image: ''
---



<p>It’s impossible to talk about how social scientists can use git without talking about project management so this will be a bit of a Frankenstein’s monster of a post.</p>
<p>In the spirit of full transparency, I have to admit that I do not use and am probably unaware of the vast majority of git functionality. I’m not a computer programmer, I’m a psychologist. I don’t really need to know how to branch down alternate development pathways. I do, however, need to stay meticulously organized.</p>
<p><img src="https://imgs.xkcd.com/comics/git.png" /> One of my goals as a scientist is to be able to forget things easily. I need a system that is flexible enough to hold data and generate analyses, reports, papers and bibliographies into web pages, pdf’s, presentations and word documents, with ease, and without my needing to remember anything about a specific project. On multiple computers. Without me being required to remember anything. Sometimes with months or even years going by between openings. With me forgetting most everything about the work. Did I mention I forget things?</p>
<p>The only way I have found to handle this is to always do everything the same way. Git can help. It promotes taking an active role in staying organized.</p>
<p>Organizations starts with names. Name your data sets the same way. Give them long names that include the date and some relevant descriptives, and don’t use any spaces. Have a format, e.g.</p>
<pre><code>Data_20170818_Projectname_Conferencesubmission.csv</code></pre>
<p>Be rigorous about that format. The first step of any project for me is to rename the dataset into a conserved format. Anything that has version 1 or version 7 or anything like that in it is useless pretty soon.</p>
<p>I don’t hold my data in the same place I hold my analyses because I have to be very careful about data set security, but a lot of recommendations advise you do so if your data allows non-secure storage. I would if I could.</p>
<p>For a specific project, I will create a single file that loads and organizes the data. That’s the only goal of the file - an R markdown document, in my case. This format is what I am most comfortable with; I have never seen the advantage of using makefiles for my particular, limited, needs.</p>
<p>Some of my best learning has occurred making the data loading file. It’s tempting, especially when you are new to coding stat programs, to go back to the raw dataset and do the transforms you need there. Learning coding stats is really, really frustrating. Make a rule for yourself not to cheat. If you have a wide form dataset and you need it to be in long form, learn how to use base R or tidyverse tools to make that happen. Go to the next stack overflow page, and the next. You’ll figure it out eventually. The magrittr pipe format has been especially helpful in creating these documents, and you can make personal rules for learning - for example, if you need to do the same operation to multiple variables, write a function, don’t copy and paste. No copying and pasting.</p>
<p>Back to git. You now have a file - a very important file. From this point forward we need to think in terms of hierarchies. This data prep file is going to be stored in the top level of the organizational hierarchy. And now we have to start thinking about relative vs absolute filepaths. If you are going to work on multiple computers you need to use relative file-paths and hold dependent documents in the same working directory as their parent documents. If not the links will break when you move across computers and you will be forever fixing file-paths in your code.</p>
<p>The next thing I will do is open a new markdown document in R and use the first code chunk to call to and execute the data prep code chunk. I use Rproject files to manage all of this within R studio - more on that later. But for now we have a 3 layer dependency:</p>
<pre><code>data set(s) -&gt; data prep file -&gt; data analysis file</code></pre>
<p>and we have two important files to keep track of and note how they grow and change (and allow for reversion if needed). Let’s say your first task in the analysis is a couple of histograms and qqplots of relevant variables so you run that code and generate html output in a browser. Now you have three files to track - the two markdown files and the html output. Chances are R generated some cache files as well so you are now looking at 4 or 5 files to track. But if you know what the point-of-focus file is - in this case, the markdown file containing the histogram and qq plot code, you have only one thing you really need to remember or know to look for, and if you need to alter the data or data loading you can backtrack. The point-of - focus analysis file is built on the data prep and data set and generates the html output but that one, single file is your real workspace, and it is simple, clean, and uncluttered.</p>
<p>Start every analysis session with a fresh pull, and end by pushing back to origin with git. If you use GitHub as origin/remote, there is another neat trick you can do. Set up a folder called docs in your project folder. Move your html output file into the docs folder and push to origin. Log in to GitHub and go to settings in the project folder. Go down to the GitHub pages section and enable the radio button to master branch/docs, turning on GitHub Pages. Now, if you were to visit the url:</p>
<p><a href="https://yourname.github.io/projectfile/projectanalysis.html" class="uri">https://yourname.github.io/projectfile/projectanalysis.html</a></p>
<p>you would find a beautifully rendered webpage of the analysis you just completed.</p>
<p>So, git has two outcomes here - forcing meticulous organization, and hosting the output.</p>
<p>There are many other uses as well, including collaboration and the ease of version tracking markdown, but for now that’s good enough.</p>
